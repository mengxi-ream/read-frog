# 使用Ollama本地部署大模型与read-frog插件联动的技术文档

## 一、为什么需要这个

### 1.1 数据隐私保障

在本地使用Ollama部署大模型，数据始终保留在本地环境中，不会像使用云服务那样需要将数据上传到第三方服务器。例如企业内部的财务数据、研发数据，或是个人的隐私信息，网页浏览信息，在与大模型交互过程中不会面临数据被第三方截取、收集或用于其他目的的风险，从而从根本上保护数据隐私，避免数据泄露隐患。

### 1.2 安全性能提升

本地部署大模型，减少了与外部网络的交互，降低了遭受网络攻击的可能性。云服务可能面临的DDoS攻击、API密钥泄露等问题，在本地部署模式下都可以得到有效规避。并且本地环境可以根据企业或个人需求，灵活设置严格的访问控制、权限管理策略，进一步保障大模型及数据的安全性 。

### 1.3 离线使用便利性

当处于网络不稳定的环境，网络卡顿时，本地部署的Ollama大模型仍能正常运行。

## 二、Windows下Ollama的安装，配置跨域支持

### 2.1 下载安装包

访问Ollama官方网站 [点此](https://ollama.ai/)

### 2.2 执行安装

请不要双击下载好的`.exe`文件安装，不然他会默认装在C盘，请打开命令行窗口，将下载好的`.exe`文件拖拽到命令行窗口中, `OllamaSetup.exe /DIR="D:\Your\Path"` 装到其他盘

### 2.3 配置跨域

请配置以下的系统环境变量

api 服务监听地址 OLLAMA_HOST=0.0.0.0

允许跨域访问 OLLAMA_ORIGINS=\*

模型文件下载位置 OLLAMA_MODELS=F:\ollama\models （这个跟跨域无关，主要是因为模型文件下载位置默认在C盘，会把你C盘撑爆，所以需要手动指定模型存放路径）

配置完成后重启电脑

## 三、Windows下Ollama常用使用命令

### 3.1 查看Ollama版本

```bash
ollama version
```

该命令用于查看当前安装的Ollama版本信息，确认安装是否成功以及了解版本情况。

### 3.2 搜索可用模型

```bash
ollama search [关键词]
```

例如，搜索与LLaMA相关的模型：

```bash
ollama search llama
```

此命令会列出符合关键词的模型列表，包括模型名称、简介等信息，方便用户选择需要的模型。

### 3.3 拉取模型

```bash
ollama pull [模型名称]
```

比如拉取`llama2`模型：

```bash
ollama pull llama2
```

执行该命令后，Ollama会从模型仓库下载指定的模型到本地，后续可离线使用。

### 3.4 运行模型

```bash
ollama run [模型名称]
```

运行`llama2`模型并与它进行交互：

```bash
ollama run llama2
```

运行后，在命令行中输入问题或指令，模型会返回相应的回答。也可以在运行时添加参数来调整模型行为，例如：

```bash
ollama run llama2 --system "你是一个专业的翻译，将用户输入翻译成英文"
```

通过`--system`参数可以为模型设置角色或任务说明。

### 3.5 停止正在运行的模型

```bash
ollama stop [模型名称]
```

若要停止运行中的`llama2`模型：

```bash
ollama stop llama2
```

此命令用于关闭正在运行的模型进程，释放系统资源。

### 3.6 列出已下载的模型

```bash
ollama list
```

该命令会显示本地已下载的所有模型列表，包括模型名称、大小、创建时间等信息，方便用户管理本地模型资源 。

### 3.7 删除模型

```bash
ollama delete [模型名称]
```

如果想删除不再使用的`llama2`模型：

```bash
ollama delete llama2
```

执行后，模型将从本地删除，释放磁盘空间。

## 四、read-frog插件中ollama使用建议

### 4.1 ollama的 api key 和 模型推荐

本地部署的ollama的地址默认在http://localhost:11434，key可以随便写，模型名称你必须与 `ollama list` 列出来的模型名称一致

建议使用gemma3:1b，模型大小为815MB，麻雀虽小五脏俱全，用来翻译效果还可以，而且系统负载还小cpu也能用。不建议用deepseek这类推理模型，因为他会将推理过程一并输出（已经做了过滤）导致翻译结果和速度 quality 都很差。

read-frog的读功能不建议用本地模型，因为本地模型会占用大量显存内存，大部分本地部署的模型上下文窗口大小支持有限，输入的内容过长，超出了模型的上下文窗口限制，Ollama 就会采取截断的方式来尝试解决这个问题，但是其实没啥用，读功能照样不能用
